{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset descargado desde\n",
    "## https://ieee-dataport.org/documents/smart-defender-dataset\n",
    "\n",
    "La descripción de los campos que componen los datos del dataset están disponible en esa URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 1: Uso de Spark para análisis de datos, Dataframes #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos las librerías que vamos a necesitar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 2.4.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://adzarei-mac.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109499bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar la versión de Spark usada\n",
    "print(\"Spark Version: \" + spark.version)\n",
    "# Datos de la sesión spark\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset en un Dataframe\n",
    "data_path = os.path.join(\"data\",\"dataset_descriptor.csv.gz\")\n",
    "cyber_df =  spark.read.option(\"header\",\"true\").csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip_proto: string (nullable = true)\n",
      " |-- ip_len_mean: string (nullable = true)\n",
      " |-- ip_len_median: string (nullable = true)\n",
      " |-- ip_len_var: string (nullable = true)\n",
      " |-- ip_len_std: string (nullable = true)\n",
      " |-- ip_len_entropy: string (nullable = true)\n",
      " |-- ip_len_cv: string (nullable = true)\n",
      " |-- ip_len_cvq: string (nullable = true)\n",
      " |-- ip_len_rte: string (nullable = true)\n",
      " |-- ip_ttl_mean: string (nullable = true)\n",
      " |-- ip_ttl_median: string (nullable = true)\n",
      " |-- ip_ttl_var: string (nullable = true)\n",
      " |-- ip_ttl_std: string (nullable = true)\n",
      " |-- ip_ttl_entropy: string (nullable = true)\n",
      " |-- ip_ttl_cv: string (nullable = true)\n",
      " |-- ip_ttl_cvq: string (nullable = true)\n",
      " |-- ip_ttl_rte: string (nullable = true)\n",
      " |-- sport_mean: string (nullable = true)\n",
      " |-- sport_median: string (nullable = true)\n",
      " |-- sport_var: string (nullable = true)\n",
      " |-- sport_std: string (nullable = true)\n",
      " |-- sport_entropy: string (nullable = true)\n",
      " |-- sport_cv: string (nullable = true)\n",
      " |-- sport_cvq: string (nullable = true)\n",
      " |-- sport_rte: string (nullable = true)\n",
      " |-- dport_mean: string (nullable = true)\n",
      " |-- dport_median: string (nullable = true)\n",
      " |-- dport_var: string (nullable = true)\n",
      " |-- dport_std: string (nullable = true)\n",
      " |-- dport_entropy: string (nullable = true)\n",
      " |-- dport_cv: string (nullable = true)\n",
      " |-- dport_cvq: string (nullable = true)\n",
      " |-- dport_rte: string (nullable = true)\n",
      " |-- tcp_seq_median: string (nullable = true)\n",
      " |-- tcp_seq_mean: string (nullable = true)\n",
      " |-- tcp_seq_var: string (nullable = true)\n",
      " |-- tcp_seq_std: string (nullable = true)\n",
      " |-- tcp_seq_entropy: string (nullable = true)\n",
      " |-- tcp_seq_cv: string (nullable = true)\n",
      " |-- tcp_seq_cvq: string (nullable = true)\n",
      " |-- tcp_seq_rte: string (nullable = true)\n",
      " |-- tcp_ack_mean: string (nullable = true)\n",
      " |-- tcp_ack_median: string (nullable = true)\n",
      " |-- tcp_ack_var: string (nullable = true)\n",
      " |-- tcp_ack_std: string (nullable = true)\n",
      " |-- tcp_ack_entropy: string (nullable = true)\n",
      " |-- tcp_ack_cv: string (nullable = true)\n",
      " |-- tcp_ack_cvq: string (nullable = true)\n",
      " |-- tcp_ack_rte: string (nullable = true)\n",
      " |-- tcp_dataofs_mean: string (nullable = true)\n",
      " |-- tcp_dataofs_median: string (nullable = true)\n",
      " |-- tcp_dataofs_var: string (nullable = true)\n",
      " |-- tcp_dataofs_std: string (nullable = true)\n",
      " |-- tcp_dataofs_entropy: string (nullable = true)\n",
      " |-- tcp_dataofs_cv: string (nullable = true)\n",
      " |-- tcp_dataofs_cvq: string (nullable = true)\n",
      " |-- tcp_dataofs_rte: string (nullable = true)\n",
      " |-- tcp_flags_mean: string (nullable = true)\n",
      " |-- tcp_flags_median: string (nullable = true)\n",
      " |-- tcp_flags_var: string (nullable = true)\n",
      " |-- tcp_flags_std: string (nullable = true)\n",
      " |-- tcp_flags_entropy: string (nullable = true)\n",
      " |-- tcp_flags_cv: string (nullable = true)\n",
      " |-- tcp_flags_cvq: string (nullable = true)\n",
      " |-- tcp_flags_rte: string (nullable = true)\n",
      " |-- tcp_window_mean: string (nullable = true)\n",
      " |-- tcp_window_median: string (nullable = true)\n",
      " |-- tcp_window_var: string (nullable = true)\n",
      " |-- tcp_window_std: string (nullable = true)\n",
      " |-- tcp_window_entropy: string (nullable = true)\n",
      " |-- tcp_window_cv: string (nullable = true)\n",
      " |-- tcp_window_cvq: string (nullable = true)\n",
      " |-- tcp_window_rte: string (nullable = true)\n",
      " |-- Label1: string (nullable = true)\n",
      " |-- Label2: string (nullable = true)\n",
      " |-- Label3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el schema heredado\n",
    "cyber_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar el número de registros del dataset\n",
    "total_count = cyber_df.count()\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http_flood\n",
      "http_slow_body\n",
      "tcp_fin_flood\n",
      "http_slow_range\n",
      "normal\n",
      "udp_flood\n",
      "http_slow_read\n",
      "tcp_syn_ack_flood\n",
      "tcp_syn_flood\n",
      "tcp_ack_flood\n",
      "http_slow_headers\n"
     ]
    }
   ],
   "source": [
    "# Q1. ¿Cuantas clases de tipo de tráfico hay clasificadas en el campo label1?\n",
    "label1_values = cyber_df.select(\"Label1\").distinct().collect()\n",
    "for label in label1_values:\n",
    "    print(label.Label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyber_df.select(\"Label1\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Label1           |\n",
      "+-----------------+\n",
      "|http_flood       |\n",
      "|http_slow_body   |\n",
      "|tcp_fin_flood    |\n",
      "|http_slow_range  |\n",
      "|normal           |\n",
      "|udp_flood        |\n",
      "|http_slow_read   |\n",
      "|tcp_syn_ack_flood|\n",
      "|tcp_syn_flood    |\n",
      "|tcp_ack_flood    |\n",
      "|http_slow_headers|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para obtener el mismo resultado que en el PDF debemos usar show(11)\n",
    "cyber_df.select(\"Label1\").distinct().show(n=11, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http_flood\n",
      "tcp_flood\n",
      "normal\n",
      "udp_flood\n",
      "http_slow\n"
     ]
    }
   ],
   "source": [
    "# Q2. ¿Cuantas clases de tipo de tráfico hay clasificadas en el campo label3?\n",
    "label3_values = cyber_df.select(\"Label3\").distinct().collect()\n",
    "for label in label3_values:\n",
    "    print(label.Label3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyber_df.select(\"Label3\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Label3    |\n",
      "+----------+\n",
      "|http_flood|\n",
      "|tcp_flood |\n",
      "|normal    |\n",
      "|udp_flood |\n",
      "|http_slow |\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para obtener el mismo resultado que en el PDF podemos usar show(5)\n",
    "cyber_df.select(\"Label3\").distinct().show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49.26 %\n"
     ]
    }
   ],
   "source": [
    "#Q3 ¿Que porcentaje de tráfico está catalogado como anormal? Entiendase por anormal aquel que no está etiquetado como normal.\n",
    "anormal_trafic_count = cyber_df.where(cyber_df.Label3 != lit(\"normal\")).count()\n",
    "anormal_trafic_percentage = (anormal_trafic_count / total_count) * 100\n",
    "print(\"% 5.2f %%\" % (anormal_trafic_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|    Label3|         percentage|\n",
      "+----------+-------------------+\n",
      "|http_flood| 0.7626373626373626|\n",
      "| tcp_flood|  32.94065934065934|\n",
      "|    normal|  50.74285714285715|\n",
      "| udp_flood| 15.151648351648353|\n",
      "| http_slow|0.40219780219780216|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4 Mostrar los porcentajes de trafico sobre el total asociados a cada tipo de etiqueta de tráfico \n",
    "# (usar el campo genérico Label3 y no el detallado Label 1)\n",
    "# Mostrar un diagrama con estos porcentajes (bar plot)\n",
    "trafic_per_label3 = cyber_df.groupBy(cyber_df.Label3).count()\n",
    "trafic_label3_percentage = trafic_per_label3.select(trafic_per_label3.Label3, ((col(\"count\") / lit(total_count)) * lit(100)).alias(\"percentage\"))\n",
    "trafic_label3_percentage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent(a, b):\n",
    "    (a / lit(b)) * lit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_udf = udf(example, LongType())\n",
    "df.withColumn('result', example_udf(df.address1, df.address2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: None of type <class 'NoneType'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d0a4606d279c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrafic_per_label3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrafic_per_label3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \"\"\"\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: None of type <class 'NoneType'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "trafic_per_label3.select(trafic_per_label3.Label3, percent(col(\"count\"), total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of DataFrame[Label3: string, count: bigint]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trafic_per_label3.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 Identificar que tipo de tráfico de red está incluido en el dataset (usar el campo ip_proto y convertir \n",
    "# ese valor al real que debería tener, es decir, un entero en el rango definido por el IANA)\n",
    "# https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 Calcular la cantidad total (suponer que el dato a aculumar para cada paquete es ip_len_mean) \n",
    "# de bytes transmitidos agrupados por protocolo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Indicar cual es tráfico anómalo en UDP que usa más tráfico de red. Presentar los resultados en orden demayor a menor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 Indicar cual es tráfico anómalo en TCP que usa más tráfico de red. Presentar los resultados en orden demayor a menor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2: Uso de Spark para análisis de datos, Spark SQL #\n",
    "\n",
    "Contestar a las cuestiones anteriores empleando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la vista en memoria y añadir una celda por cada cuestión que se debe responder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. ¿Cuantas clases de tipo de tráfico hay clasificadas en el campo label1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. ¿Cuantas clases de tipo de tráfico hay clasificadas en el campo label3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 ¿Que porcentaje de tráfico está catalogado como anormal? Entiendase por anormal aquel que no está etiquetado como normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 Mostrar los porcentajes de trafico sobre el total asociados a cada tipo de etiqueta de tráfico \n",
    "# (usar el campo genérico Label3 y no el detallado Label 1)\n",
    "# Mostrar un diagrama con estos porcentajes (bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 Identificar que tipo de tráfico de red está incluido en el dataset (usar el campo ip_proto y convertir \n",
    "# ese valor al real que debería tener, es decir, un entero en el rango definido por el IANA)\n",
    "# https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 Calcular la cantidad total (suponer que el dato a aculumar para cada paquete es ip_len_mean) \n",
    "# de bytes transmitidos agrupados por protocolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 Indicar cual es tráfico anómalo en UDP que usa más tráfico de red. Presentar los resultados en orden demayor a menor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 Indicar cual es tráfico anómalo en TCP que usa más tráfico de red. Presentar los resultados en orden demayor a menor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
