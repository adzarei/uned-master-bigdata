{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con ventanas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming proporciona las siguientes operaciones con ventanas deslizantes sobre los datos. Los parámetros más importantes a la hora de definir la ventana son los siguientes:\n",
    "\n",
    "* window length - Duración de la ventana en segundos.\n",
    "* sliding interval - Intervalo de desplazamiento de la ventana en segundos.\n",
    "\n",
    "Ambos parámetros han de ser múltiplos de la duración del batch del DStream original.\n",
    "\n",
    "\n",
    "| Método        | Funcionamiento           |\n",
    "| -------------:|:-------------|\n",
    "| **window**(windowLength, slideInterval)      | Nuevo DStream calculado según batches en ventanas del DStream original, tamaño de ventana *windowLength* y salto entre ventanas *slideInterval*. |\n",
    "| **countByWindow**(windowLength, slideInterval)     | Devuelve el conteo de elementos de la ventana deslizante definida con *windowLength* y *slideInterval*, sobre el DStream original.    |\n",
    "| **reduceByWindow**(func, windowLength, slideInterval) | Similar a reduce, permite la agregación de elementos de cada RDD del DStream original para generar un nuevo DStream con RDDs simples, sobre los batches de la ventana deslizante definida con *windowLength* y *slideInterval*. La función *func* ha de ser conmutativa y asociativa, recibir dos argumentos y devolver uno.     |\n",
    "| **reduceByKeyAndWindow**(func, windowLength, slideInterval, [numTasks])     | Similar a reduceByKey, sobre un Dstream original con pares clave-valor (K,V), devuelve el agregado según la función *func* para cada clave, sobre los *batches* de la ventana deslizante definida con *windowLength* y *slideInterval*. Número de tareas paralelas opcional.  |\n",
    "| **countByValueAndWindow**(windowLength, slideInterval, [numTasks]) | Sobre un DStream de pares clave-valor (K,V), devuelve la frecuencia de cada clave en formato (K, Long), sobre los *batches* de la ventana deslizante definida con *windowLength* y *slideInterval*. Número de tareas paralelas opcional.      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#Importante: Modificar la ruta para que apunte al HOME de Spark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[2]\", appName=\"WindowWordcount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "#El uso de checkpoints es necesario cuando se van a realizar operaciones con ventanas\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción de datos\n",
    "Se va a utilizar un socket para introducir datos. Dicho socket estará escuchando en el puerto 9999 de localhost. Para la introducción de datos se puede abrir una ventana de comandos y ejecutar el comando \"nc -lk 9999\", que abre el puerto y lo mantiene abierto, para a continuación pegar los mensajes que se quieren enviar. Si no existe el comando, deberemos instalar el programa \"netcat\" --> \"sudo apt-get install netcat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se van a probar tres usos de ventanas distintas (recomendable sólo ejecutar una transformación cada vez para evitar comportamientos erróneos, es decir, comentar las otras dos transformaciones):\n",
    "- En la primera, se muestran las palabras que aparecen en cada ventana, sin agrupar (se repiten las palabras).\n",
    "- En la segunda, se cuenta el número de palabras totales en cada ventana (countByWindow).\n",
    "- En la tercera, se muestra el número de veces que aparece cada palabra en cada ventana (reduceByKeyAndWindow).\n",
    "\n",
    "Introducir cualquier combinación de letras, palabras o frases, dejando un tiempo entre una frase y otra. Para comprobar la tercera transformación, procurar que en la misma ventana de 5 segundos haya palabras que aparezcan más de una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.strip().split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "#Las ventanas muestran los valores correspondientes a los últimos 5 segundos (con un desplazamiento de un segundo).\n",
    "pairs.window(5, 1).pprint()\n",
    "pairs.countByWindow(5, 1).pprint()\n",
    "pairs.reduceByKeyAndWindow(lambda x,y:(x+y), 5, 1).pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
