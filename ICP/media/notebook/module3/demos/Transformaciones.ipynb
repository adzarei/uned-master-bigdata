{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los DStreams en Spark Streaming se tratan de forma similar a los RDDs cuando trabajamos con Spark. Existen muchas opciones para realizar transformaciones sobre los datos:\n",
    "\n",
    "| Transformation        | Meaning         |\n",
    "| ------------------------------ |:-------------|\n",
    "| **map**(func)      | Se aplica la función func a cada elemento de DStream. Devuelve un nuevo DStream.    |\n",
    "| **flatMap**(func)\t| Similar a map, pero por cada elemento de entrada se devuelven de 0 a n elementos de salida.    |\n",
    "| **filter**(func)\t| Nuevo DStream con los elementos del DStream original para los que func devuelva True    |\n",
    "| **repartition**(numPartitions)\t| Modifica el nivel de paralelismo de un DStream.    |\n",
    "| **union**(otherStream)\t| Nuevo DStream con la unión del actual y de otherStream. |\n",
    "| **count**()\t| Nuevo DStream con RDDs simples con el número de elementos de cada RDD en el DStream original  |\n",
    "| **reduce**(func)\t| Agregación de elementos de cada RDD del DStream original para generar un nuevo DStream con RDDs simples. La función func ha de ser conmutativa y asociativa, recibir dos argumentos y devolver uno. |\n",
    "| **countByValue**()\t| Sobre un DStream de elementos de tipo K, devuelve la frecuencia de cada elemento del DStream en formato (K, Long). |\n",
    "| **reduceByKey**(func, [numTasks])\t|Sobre un DStream de elementos clave-valor (K,V), devuelve la agregación de todos los valores con la misma clave, según la función func. Se puede especificar el número de tareas paralelas de forma opcional (por defecto 2). |\n",
    "| **join**(otherStream, [numTasks])\t| Sobre dos DStreams (original y otherStream) de clave-valor (K,V) y (K,W), devuelve un DStream (K, (V,W)), sólo para aquéllas claves que aparezcan en ambos DStreams. Número de tareas paralelas opcional. |\n",
    "| **cogroup**(otherStream, [numTasks])\t| Sobre dos DStreams (original y otherStream) de clave-valor (K,V) y (K,W), devuelve un nuevo DStream con tuplas (K, Seq[V], Seq[W]), para todas las claves que aparezcan al menos en uno de los DStreams. Número de tareas paralelas opcional. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# Importante: Modificar la ruta para que apunte al HOME de Spark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se especifica el contexto Spark y el contexto Streaming definiendo la duración del batch (en este caso, 4 segundos)\n",
    "sc = SparkContext(master=\"local[2]\", appName=\"EjemploTransformaciones\")\n",
    "scc = StreamingContext(sc, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción de datos\n",
    "Se va a utilizar un socket para introducir datos. Dicho socket estará escuchando en el puerto 9999 de localhost. Para la introducción de datos se puede abrir una ventana de comandos y ejecutar el comando \"nc -lk 9999\", que abre el puerto y lo mantiene abierto, para a continuación pegar los mensajes que se quieren enviar. Si no existe el comando, deberemos instalar el programa \"netcat\" --> \"sudo apt-get install netcat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = scc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se van a probar cuatro transformaciones distintas:\n",
    "- En la primera, se eliminan signos de puntuación del texto introducido, y se pasa a minúsculas.\n",
    "- En la segunda, se utiliza un flatMap para que cada elemento del RDD dentro del DStream sea una palabra.\n",
    "- En la tercera, se filtran aquéllas palabras que sean números del 0 al 9.\n",
    "- En la cuarta, se realiza un conteo de la frecuencia de cada palabra, para cada batch de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede probar a introducir el siguiente texto (varias líneas):\n",
    "   \n",
    "Esta es la línea 1.  \n",
    "Esta es la línea 2.  \n",
    "Esta es la línea 3.   \n",
    "Otra línea más, esta sin número.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations1 = lines.map(lambda x: x.replace(',',' ').replace('?',' ').replace(':',' ').replace('.',' ').replace('-',' ').lower())\n",
    "transformations1.pprint()\n",
    "transformations2 = transformations1.flatMap(lambda line: line.split(\" \"))\n",
    "transformations2.pprint()\n",
    "stringNumbers = {'1','2','3','4','5','6','7','8','9','0'}\n",
    "transformations3 = transformations2.filter(lambda x: x not in stringNumbers)\n",
    "transformations3.pprint()\n",
    "transformations4 = transformations3.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y)\n",
    "transformations4.pprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante start() ejecutamos el programa, y lo detenemos cuando queramos con stop()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
