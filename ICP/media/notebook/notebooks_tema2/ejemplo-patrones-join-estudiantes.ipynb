{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de patrones: Reunión (join)\n",
    "\n",
    "En este ejercicio vamos a realizar la reunión (o join) de dos datasets. En primer lugar tenemos el fichero de posts con el que hemos trabajado anteriormente, y por otro lado tenemos otro fichero que contiene información de los usuarios. Sobre el fichero de los usuarios, tiene una serie de campos separados por tabuladores. Los campos son los siguientes: \n",
    "\n",
    "\"user_ptr_id\", \"reputation\", \"gold\", \"silver\", \"bronze\" \n",
    "\n",
    "El primer campo almacena un identificador para el usuario, mientras que los siguientes almacenan una serie de valores que sirven para calificar al usuario en función de sus contribuciones en el foro.\n",
    "\n",
    "El resultado de la ejecución de este programa MapReduce debe ser que, para cada post del foro, se muestren los siguientes datos: \n",
    "\n",
    "\"id\", \"title\", \"tagnames\", \"author_id\", \"added_at\", \"score\", \"user_ptr_id\", \"reputation\", \"gold\", \"silver\", \"bronze\" \n",
    "\n",
    "De estos datos, los campos \"id\", \"title\", \"tagnames\", \"author_id\", \"added_at\", y  \"score\" vienen del fichero que contiene los posts y el resto de los campos vienen del fichero de los usuarios.  \n",
    "\n",
    "Para una posible implementación es de interés incluir en la salida del map un nuevo campo sintético, que sirva para ordenar la información de un usuario y los posts que ha publicado. Este campo puede tomar el valor “A” para la información de usuarios y “B” para la información de los posts. De esta forma, los reducers recibirán, para cada usuario, primero su información de usuario y seguidamente sus posts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ejemplo-patrones/join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/ejemplo-patrones/join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/ejemplo-patrones/join\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    " \n",
    "reader = csv.reader(sys.stdin, delimiter='\\t')\n",
    "writer = csv.writer(sys.stdout, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# PON TU CÓDIGO AQUÍ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# Los valores que contengan como source A son los datos de usuarios, y B datos de posts\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "\n",
    "reader = csv.reader(sys.stdin, delimiter='\\t')\n",
    "writer = csv.writer(sys.stdout, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "currentUser = None\n",
    "\n",
    "id = title = tagnames = author_id = parent_id = abs_parent_id = added_at = score = reputation = gold = silver = bronze = \"\"\n",
    "source = None\n",
    "# PON TU CÓDIGO AQUÍ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 root supergroup       1774 2019-08-12 08:03 /tmp/forumdata/forum1.tsv\r\n",
      "-rw-r--r--   3 root supergroup  120313812 2019-08-12 08:03 /tmp/forumdata/forum_node.tsv\r\n",
      "-rw-r--r--   3 root supergroup     530989 2019-08-12 08:03 /tmp/forumdata/forum_users.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/forumdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/salida-join/*': No such file or directory\n",
      "rmdir: `/tmp/salida-join': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/salida-join/*\n",
    "! hdfs dfs -rmdir /tmp/salida-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob5400627880740110649.jar tmpDir=null\n",
      "19/08/12 10:04:42 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "19/08/12 10:04:42 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "19/08/12 10:04:42 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "19/08/12 10:04:42 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "19/08/12 10:04:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1565590814059_0019\n",
      "19/08/12 10:04:43 INFO impl.YarnClientImpl: Submitted application application_1565590814059_0019\n",
      "19/08/12 10:04:43 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1565590814059_0019/\n",
      "19/08/12 10:04:43 INFO mapreduce.Job: Running job: job_1565590814059_0019\n",
      "19/08/12 10:04:48 INFO mapreduce.Job: Job job_1565590814059_0019 running in uber mode : false\n",
      "19/08/12 10:04:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/08/12 10:04:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/08/12 10:05:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/08/12 10:05:01 INFO mapreduce.Job: Job job_1565590814059_0019 completed successfully\n",
      "19/08/12 10:05:01 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17736295\n",
      "\t\tFILE: Number of bytes written=36062811\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=120846897\n",
      "\t\tHDFS: Number of bytes written=19688361\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14973\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3213\n",
      "\t\tTotal time spent by all map tasks (ms)=14973\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3213\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14973\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3213\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15332352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3290112\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=940178\n",
      "\t\tMap output records=222224\n",
      "\t\tMap output bytes=17282511\n",
      "\t\tMap output materialized bytes=17736307\n",
      "\t\tInput split bytes=304\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=17607\n",
      "\t\tReduce shuffle bytes=17736307\n",
      "\t\tReduce input records=222224\n",
      "\t\tReduce output records=204617\n",
      "\t\tSpilled Records=444448\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=413\n",
      "\t\tCPU time spent (ms)=6990\n",
      "\t\tPhysical memory (bytes) snapshot=1133674496\n",
      "\t\tVirtual memory (bytes) snapshot=10557665280\n",
      "\t\tTotal committed heap usage (bytes)=1040711680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=120846593\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=19688361\n",
      "19/08/12 10:05:01 INFO streaming.StreamJob: Output directory: /tmp/salida-join\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /tmp/forumdata/forum_node.tsv -input /tmp/forumdata/forum_users.tsv \\\n",
    "-output /tmp/salida-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\t\"0\"\t\"4\"\r\n",
      "\"12003576\"\t\"\"\t\"cs258 \"\t\"100119321\"\t\"2012-08-16 21:54:08.588876+00\"\t\"1\"\t\"76\"\t\"0\"\t\"0\"\t\"4\"\r\n",
      "\"12003561\"\t\"You should add an item to the returned list each time enqueue() or dequeue() is called.\"\t\"cs258 spoiler final-2 meta\"\t\"100119321\"\t\"2012-08-16 04:36:19.794722+00\"\t\"0\"\t\"76\"\t\"0\"\t\"0\"\t\"4\"\r\n",
      "\"12003489\"\t\"\"\t\"cs258 \"\t\"100119376\"\t\"2012-08-10 22:22:06.93941+00\"\t\"0\"\t\"1\"\t\"0\"\t\"0\"\t\"1\"\r\n",
      "\"12003513\"\t\"type of testing\"\t\"cs258 unit1-9 m-48720273\"\t\"100119386\"\t\"2012-08-12 05:14:14.80794+00\"\t\"1\"\t\"1\"\t\"0\"\t\"0\"\t\"1\"\r\n",
      "\"12003589\"\t\"\"\t\"cs258 \"\t\"100119409\"\t\"2012-08-19 22:21:12.14117+00\"\t\"0\"\t\"41\"\t\"0\"\t\"0\"\t\"2\"\r\n",
      "\"12003569\"\t\"\"\t\"cs258 \"\t\"100119415\"\t\"2012-08-16 15:03:46.57102+00\"\t\"0\"\t\"41\"\t\"0\"\t\"0\"\t\"2\"\r\n",
      "\"12003565\"\t\"\"\t\"cs258 \"\t\"100119430\"\t\"2012-08-16 13:08:25.856929+00\"\t\"0\"\t\"71\"\t\"0\"\t\"0\"\t\"3\"\r\n",
      "\"12003557\"\t\"\"\t\"cs258 \"\t\"100119430\"\t\"2012-08-15 15:47:49.224421+00\"\t\"0\"\t\"71\"\t\"0\"\t\"0\"\t\"3\"\r\n",
      "\"12003566\"\t\"\"\t\"cs258 \"\t\"100119430\"\t\"2012-08-16 13:10:39.810898+00\"\t\"2\"\t\"71\"\t\"0\"\t\"0\"\t\"3\"\r\n",
      "\"12003564\"\t\"\"\t\"cs258 \"\t\"100119430\"\t\"2012-08-16 12:47:39.764575+00\"\t\"0\"\t\"71\"\t\"0\"\t\"0\"\t\"3\"\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/salida-join/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
