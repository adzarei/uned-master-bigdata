{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrjob: Muestras\n",
    "\n",
    "En este ejemplo vamos a ver dos conceptos importantes de mrjob: los protocolos y el paso de parámetros.  En primer lugar veremos qué son los protocolos. Mrjob asume que todos los datos son bytes delimitados por saltos de línea, y los serializa y deserializa utilizando protocolos. Cada trabajo mrjob tiene un protocolo de entrada, otro de salida y otro interno.  \n",
    "\n",
    "Un protocolo tiene un método read() y otro write(). El read() convierte los bytes a pares de objetos Python que representan las claves y los valores. El método write() a su vez convierte un par de objetos de Python en bytes.  \n",
    "\n",
    "El protocolo de entrada se utiliza para leer los bytes que recibe el primer paso (el primer map, o el primer reduce si el primer paso no tiene map). El protocolo de salida se utiliza para escribir la salida del último paso a bytes que se escribirán en el fichero de salida. El protocolo interno convierte la salida de un paso en la entrada del siguiente paso si el trabajo tiene más de un paso. \n",
    "\n",
    "El protocolo de entrada por defecto es RawValueProtocol, que simplemente lee una línea como un str. Por tanto, el primer paso del trabajo mrjob ve (None, línea) para cada línea de entrada. \n",
    "\n",
    "Los protocolos interno y de salida por defecto en mrjob son ambos JSONProtocol, que leen y escriben cadenas JSON separadas por un tabulador (por defecto, Hadoop Streaming utiliza el tabulador para separar claves y valores en una línea cuando ordena los datos). \n",
    "\n",
    "Para entenderlo de forma sencilla, se utiliza RawValueProtocol para leer o escribir líneas de texto en bruto, y JSONProtocol para leer o escribir pares clave-valor donde la clave y el valor son JSON.  \n",
    "\n",
    "En el siguiente enlace se muestran los diferentes protocolos que permite mrjob: \n",
    "\n",
    "https://pythonhosted.org/mrjob/protocols.html#module-mrjob.protocol \n",
    "\n",
    "En este ejercicio vemos también la forma de pasar parámetros de entrada a nuestro trabajo mrjob. El ejercicio consiste en desarrollar un código que tome como entrada un fichero de texto y devuelva como salida una muestra de sus líneas, tantas como se indique como parámetro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/muestras\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio.py\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol, ReprProtocol\n",
    "\n",
    "FACTOR_MUESTREO = 1.2\n",
    "\n",
    "class MRMuestreo(MRJob):\n",
    "    \n",
    "    # Utilizamos RawValueProtocol para la salida para devolver lineas en bruto  \n",
    "    # en lugar de pares (clave, valor) \n",
    "    OUTPUT_PROTOCOL = RawValueProtocol \n",
    "    # OUTPUT_PROTOCOL = ReprProtocol # Prueba a comentar y descomentar estas lineas para ver el efecto sobre la salida\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super(MRMuestreo, self).configure_args()\n",
    "        self.add_passthru_arg(\n",
    "            '--sample-size',\n",
    "            type=float,\n",
    "            help='Numero muestras que retornar.'\n",
    "        )\n",
    "        self.add_passthru_arg(\n",
    "            '--expected-length',\n",
    "            type=float,\n",
    "            help=(\"Numero of entries you expect in the log. If not specified,\"\n",
    "                  \" we'll pass every line to the reducer.\")\n",
    "        )\n",
    "\n",
    "    def load_args(self, args):\n",
    "        super(MRMuestreo, self).load_args(args)\n",
    "\n",
    "        if self.options.sample_size is None:\n",
    "            self.option_parser.error('You must specify the --sample-size')\n",
    "        else:\n",
    "            self.sample_size = self.options.sample_size\n",
    "\n",
    "        # Si hemos incluido una longitud estimada, podemos calcular la  \n",
    "        # probabilidad de muestreo para el mapper, de forma que el  \n",
    "        # reducer no tiene que recibir todas las entradas \n",
    "        if self.options.expected_length is None:\n",
    "            self.sampling_probability = 1.\n",
    "        else:\n",
    "\n",
    "            self.sampling_probability = (self.sample_size *\n",
    "                                         FACTOR_MUESTREO /\n",
    "                                         self.options.expected_length)\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        \"\"\" \n",
    "        Para cada linea, con probabilidad self.sampling_probability, \n",
    "        se emite una clave None, y (random, linea) como valor, de forma que \n",
    "        los valores se ordenen aleatoriamente y se envien a un unico reducer \n",
    "\n",
    "        Argumentos: \n",
    "            line - linea en bruto \n",
    "        Emite: \n",
    "            key - None \n",
    "            value - (random seed, line) \n",
    "        \"\"\" \n",
    "\n",
    "        if random.random() < self.sampling_probability:\n",
    "            seed = '%20i' % random.randint(0, sys.maxsize)\n",
    "            yield None, (seed, line)\n",
    "\n",
    "    def reducer(self, _, values):\n",
    "        \"\"\" \n",
    "        Los valores tienen asignado un numero aleatorio,  \n",
    "        Llegaran en orden aleatorio, asi que se emiten las primeras n lineas. \n",
    "        Argumentos: \n",
    "            values - generador de pares (random_seed, linea)  \n",
    "        Emite: \n",
    "            key - None \n",
    "            value - muestra aleatoria de lineas del fichero \n",
    "        \"\"\" \n",
    "\n",
    "        for line_num, (seed, line) in enumerate(values):\n",
    "            yield None, line\n",
    "\n",
    "            # enumerate() comienza en 0, asi que sumamos 1\n",
    "            if line_num + 1 >= self.sample_size:\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMuestreo.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302272 /media/notebooks/marktwain.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Averiguamos cuantas lineas tiene el fichero para que sirve como parámetro del programa\n",
    "! wc -l /media/notebooks/marktwain.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20190812.105554.161994\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/mrjob-ejercicio.root.20190812.105554.161994/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio.root.20190812.105554.161994/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20190812.105554.161994...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py /media/notebooks/marktwain.txt \\\n",
    "--sample-size 3 --expected-length 302272 > ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rage into which they lashed themselves; and, secondly, the puerility of\r\n",
      "and then it solidified itself, and you could have walked upon\r\n",
      "once, and it went by very dim to the sight and floated noiseless through\r\n"
     ]
    }
   ],
   "source": [
    "! tail ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/mrjob-muestras-output/_SUCCESS\r\n",
      "Deleted /tmp/carpeta/mrjob-muestras-output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/carpeta/mrjob-muestras-output/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-muestras-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20190812.105755.193365\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.105755.193365/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.105755.193365/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob5467777747702039131.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1565590814059_0025\n",
      "  Submitted application application_1565590814059_0025\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1565590814059_0025/\n",
      "  Running job: job_1565590814059_0025\n",
      "  Job job_1565590814059_0025 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1565590814059_0025 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-muestras-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=16018031\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=205\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=457\n",
      "\t\tFILE: Number of bytes written=446197\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=16018227\n",
      "\t\tHDFS: Number of bytes written=205\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6240256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2593792\n",
      "\t\tTotal time spent by all map tasks (ms)=6094\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6094\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2533\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2533\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6094\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2533\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1970\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=185\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=302272\n",
      "\t\tMap output bytes=441\n",
      "\t\tMap output materialized bytes=463\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=762892288\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=463\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=720896000\n",
      "\t\tVirtual memory (bytes) snapshot=7917015040\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-muestras-output\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.105755.193365...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20190812.105755.193365...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py hdfs:///tmp/carpeta/marktwain.txt --sample-size 3 \\\n",
    "--expected-length 302272 --output-dir hdfs:///tmp/carpeta/mrjob-muestras-output \\\n",
    "-r hadoop --python-bin /opt/anaconda/bin/python3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     elephant went west.  Shall now shadow him in that direction.\t\r\n",
      "of monotony, and keep it up till the monotonies ran out, if it was a\t\r\n",
      "I said I did not know, but we could try a dog and see.  So he sent\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/carpeta/mrjob-muestras-output/part-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
