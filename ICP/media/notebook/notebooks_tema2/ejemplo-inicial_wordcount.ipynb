{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo inicial: Contador de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este ejemplo trabajaremos con un fichero de texto cargado previamente en HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ejemplo-inicial/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/ejemplo-inicial/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/ejemplo-inicial/wordcount\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# entrada de la entrada estandar STDIN\n",
    "for line in sys.stdin:\n",
    "  # eliminamos espacios blancos al principio y final\n",
    "  line = line.strip()\n",
    "  # dividimos la linea en palabras\n",
    "  words = line.split()\n",
    "  # incrementamos los contadores\n",
    "  for word in words:\n",
    "    # escribimos los resultados a la salida estandard STDOUT. \n",
    "    # Esta salida sera la entrada para el reduce, es decir, para reducer.py\n",
    "    # delimiado por tab, para cada palabra ponemos 1 ocurrencia\n",
    "    print  (word +\"\\t\" + str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# entrada desde STDIN\n",
    "for line in sys.stdin:\n",
    "  # eliminamos espacios blancos al principio y final\n",
    "  line = line.strip()\n",
    "\n",
    "  # parseamos la entrada que hemos obtenido del mapper.py\n",
    "  word, count = line.split('\\t', 1)\n",
    "\n",
    "  # pasamos el contador de string a int\n",
    "  try:\n",
    "    count = int(count)\n",
    "  except ValueError:\n",
    "    # si el contados no es un numero, descartamos la linea\n",
    "    continue\n",
    "\n",
    "  # este if solamente funciona porque Hadoop ordena la salida del map por la clave (aqui es word) antes de pasarsela al reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word:\n",
    "      # escribir resultado a STDOUT\n",
    "      print (current_word+ \"\\t\" + str(current_count))\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "\n",
    "# escribimos la ultima palabra\n",
    "if current_word == word:\n",
    "    print (current_word + \"\\t\" + str(current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 22M\r\n",
      "-rwxrwxrwx 1 root root  520 Jul 10 16:38 mapper.py\r\n",
      "-rwxrwxrwx 1 root root  927 Jul 10 16:38 reducer.py\r\n",
      "-rwxrwxrwx 1 root root 1.9M Jul 10 14:59 salidawordcount\r\n",
      "-rw-r--r-- 1 root root  21M Jul 10 14:56 salidawordcountmap\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero probamos solamente la funciÃ³n map ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ../../marktwain.txt | python mapper.py  > salidawordcountmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe\t1\r\n",
      "to\t1\r\n",
      "our\t1\r\n",
      "email\t1\r\n",
      "newsletter\t1\r\n",
      "to\t1\r\n",
      "hear\t1\r\n",
      "about\t1\r\n",
      "new\t1\r\n",
      "eBooks.\t1\r\n"
     ]
    }
   ],
   "source": [
    "! tail salidawordcountmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/salida-wordcountmap/_SUCCESS\r\n",
      "Deleted /tmp/salida-wordcountmap/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/salida-wordcountmap/*\n",
    "! hdfs dfs -rmdir /tmp/salida-wordcountmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob2283734107267297335.jar tmpDir=null\n",
      "19/07/10 16:39:03 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "19/07/10 16:39:03 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "19/07/10 16:39:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/07/10 16:39:03 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/07/10 16:39:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1562757543411_0018\n",
      "19/07/10 16:39:04 INFO impl.YarnClientImpl: Submitted application application_1562757543411_0018\n",
      "19/07/10 16:39:04 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1562757543411_0018/\n",
      "19/07/10 16:39:04 INFO mapreduce.Job: Running job: job_1562757543411_0018\n",
      "19/07/10 16:39:09 INFO mapreduce.Job: Job job_1562757543411_0018 running in uber mode : false\n",
      "19/07/10 16:39:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/07/10 16:39:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/07/10 16:39:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/07/10 16:39:24 INFO mapreduce.Job: Job job_1562757543411_0018 completed successfully\n",
      "19/07/10 16:39:24 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26575151\n",
      "\t\tFILE: Number of bytes written=53510820\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=15715955\n",
      "\t\tHDFS: Number of bytes written=20990911\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10849\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5540\n",
      "\t\tTotal time spent by all map tasks (ms)=10849\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5540\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10849\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5540\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11109376\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5672960\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=302272\n",
      "\t\tMap output records=2792117\n",
      "\t\tMap output bytes=20990911\n",
      "\t\tMap output materialized bytes=26575157\n",
      "\t\tInput split bytes=196\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=163066\n",
      "\t\tReduce shuffle bytes=26575157\n",
      "\t\tReduce input records=2792117\n",
      "\t\tReduce output records=2792117\n",
      "\t\tSpilled Records=5584234\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=189\n",
      "\t\tCPU time spent (ms)=10740\n",
      "\t\tPhysical memory (bytes) snapshot=867373056\n",
      "\t\tVirtual memory (bytes) snapshot=7963475968\n",
      "\t\tTotal committed heap usage (bytes)=800063488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=15715759\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20990911\n",
      "19/07/10 16:39:24 INFO streaming.StreamJob: Output directory: /tmp/salida-wordcountmap\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-files mapper.py -mapper mapper.py \\\n",
    "-input /tmp/carpeta/marktwain.txt -output /tmp/salida-wordcountmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zigzag\t1\r\n",
      "zigzag\t1\r\n",
      "zigzag\t1\r\n",
      "zigzag;\t1\r\n",
      "zigzagging\t1\r\n",
      "zinc\t1\r\n",
      "zinc\t1\r\n",
      "zinc-plated\t1\r\n",
      "zinc-plated,\t1\r\n",
      "zis\t1\r\n",
      "zis\t1\r\n",
      "zis\t1\r\n",
      "zis\t1\r\n",
      "zis\t1\r\n",
      "zither\t1\r\n",
      "zo\t1\r\n",
      "zo\t1\r\n",
      "zo\t1\r\n",
      "zo\t1\r\n",
      "zone\t1\r\n",
      "zone\t1\r\n",
      "zone\t1\r\n",
      "zone\t1\r\n",
      "zone!\t1\r\n",
      "zones\t1\r\n",
      "zones\t1\r\n",
      "zones\t1\r\n",
      "zones\t1\r\n",
      "zoological\t1\r\n",
      "zoological\t1\r\n",
      "zoological\t1\r\n",
      "zoological\t1\r\n",
      "zoological\t1\r\n",
      "zoology,\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zu\t1\r\n",
      "zuruck,\t1\r\n",
      "zuruck.\t1\r\n",
      "zuruckkommen\t1\r\n",
      "zusammen\t1\r\n",
      "zusammengetroffen\t1\r\n",
      "zusammengetroffen,\t1\r\n",
      "zuweilen\t1\r\n",
      "zwar\t1\r\n",
      "zwei\t1\r\n",
      "zwei\t1\r\n",
      "zwei\t1\r\n",
      "zwolf\t1\r\n",
      "zylobalsamum\t1\r\n",
      "zzip!--let\t1\r\n",
      "{10}\t1\r\n",
      "{10}\t1\r\n",
      "{13}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{1}\t1\r\n",
      "{2}\t1\r\n",
      "{2}--Sir\t1\r\n",
      "{3}\t1\r\n",
      "{3}\t1\r\n",
      "{4}\t1\r\n",
      "{4}\t1\r\n",
      "{5}\t1\r\n",
      "{5}\t1\r\n",
      "{6}\t1\r\n",
      "{6}\t1\r\n",
      "{7}\t1\r\n",
      "{7}\t1\r\n",
      "{8}\t1\r\n",
      "{8}\t1\r\n",
      "{9}\t1\r\n",
      "{9}\t1\r\n",
      "{footnote\t1\r\n",
      "{footnote\t1\r\n",
      "{footnote\t1\r\n",
      "{footnote\t1\r\n",
      "{footnote\t1\r\n",
      "{footnote\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|\t1\r\n",
      "|__________________________________|\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||\t1\r\n",
      "||=======|====\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/salida-wordcountmap/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ../../marktwain.txt | python mapper.py | sort | python reducer.py > salidawordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwischen\t1\r\n",
      "zwolf\t1\r\n",
      "Zylo,\t1\r\n",
      "zylobalsamum\t1\r\n",
      "\"Zylobalsamum.\"\t1\r\n",
      "Zylobalsamum--\"\t1\r\n",
      "Zylobalsamum\t1\r\n",
      "Zylo--what\t1\r\n",
      "zzip!--let\t1\r\n",
      "--zzz--zzz--\t1\r\n"
     ]
    }
   ],
   "source": [
    "! tail salidawordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/salida-wordcount/_SUCCESS\r\n",
      "Deleted /tmp/salida-wordcount/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/salida-wordcount/*\n",
    "! hdfs dfs -rmdir /tmp/salida-wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.0.jar] /tmp/streamjob667315733110171432.jar tmpDir=null\n",
      "19/07/10 16:42:00 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "19/07/10 16:42:00 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "19/07/10 16:42:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/07/10 16:42:00 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/07/10 16:42:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1562757543411_0019\n",
      "19/07/10 16:42:01 INFO impl.YarnClientImpl: Submitted application application_1562757543411_0019\n",
      "19/07/10 16:42:01 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1562757543411_0019/\n",
      "19/07/10 16:42:01 INFO mapreduce.Job: Running job: job_1562757543411_0019\n",
      "19/07/10 16:42:06 INFO mapreduce.Job: Job job_1562757543411_0019 running in uber mode : false\n",
      "19/07/10 16:42:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/07/10 16:42:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/07/10 16:42:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/07/10 16:42:23 INFO mapreduce.Job: Job job_1562757543411_0019 completed successfully\n",
      "19/07/10 16:42:23 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=26575151\n",
      "\t\tFILE: Number of bytes written=53513658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=15715955\n",
      "\t\tHDFS: Number of bytes written=1936539\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11445\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6602\n",
      "\t\tTotal time spent by all map tasks (ms)=11445\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6602\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11445\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6602\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11719680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6760448\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=302272\n",
      "\t\tMap output records=2792117\n",
      "\t\tMap output bytes=20990911\n",
      "\t\tMap output materialized bytes=26575157\n",
      "\t\tInput split bytes=196\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=163066\n",
      "\t\tReduce shuffle bytes=26575157\n",
      "\t\tReduce input records=2792117\n",
      "\t\tReduce output records=163066\n",
      "\t\tSpilled Records=5584234\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=168\n",
      "\t\tCPU time spent (ms)=10850\n",
      "\t\tPhysical memory (bytes) snapshot=828690432\n",
      "\t\tVirtual memory (bytes) snapshot=7968071680\n",
      "\t\tTotal committed heap usage (bytes)=859308032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=15715759\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1936539\n",
      "19/07/10 16:42:23 INFO streaming.StreamJob: Output directory: /tmp/salida-wordcount\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /tmp/carpeta/marktwain.txt -output /tmp/salida-wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 root supergroup          0 2019-07-10 16:42 /tmp/salida-wordcount/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup    1936539 2019-07-10 16:42 /tmp/salida-wordcount/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/salida-wordcount/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outh--said\t1\r\n",
      "youth--the\t1\r\n",
      "youth.\t15\r\n",
      "youth;\t5\r\n",
      "youth?\t1\r\n",
      "youthful\t21\r\n",
      "youthful,\t1\r\n",
      "youthful;\t1\r\n",
      "youthfullest\t1\r\n",
      "youths\t18\r\n",
      "youths,\t6\r\n",
      "yow\t1\r\n",
      "yr's\t1\r\n",
      "yrs,\t1\r\n",
      "yrs.\t1\r\n",
      "ys\t1\r\n",
      "yt\t31\r\n",
      "yt,\t1\r\n",
      "ytt\t2\r\n",
      "yuther\t11\r\n",
      "z,\t1\r\n",
      "z9o.5.\t1\r\n",
      "zanier,\t1\r\n",
      "zareba\t1\r\n",
      "zartlichsten\t2\r\n",
      "zat\t1\r\n",
      "ze\t21\r\n",
      "zeal\t20\r\n",
      "zeal,\t6\r\n",
      "zeal--\t1\r\n",
      "zeal--in\t1\r\n",
      "zeal.\t5\r\n",
      "zealous\t9\r\n",
      "zealous--are\t1\r\n",
      "zealously\t4\r\n",
      "zebras\t1\r\n",
      "zebras--all\t1\r\n",
      "zedoary,\t1\r\n",
      "zehn\t1\r\n",
      "zenana\t4\r\n",
      "zenana,\t1\r\n",
      "zenana.\t1\r\n",
      "zenith\t9\r\n",
      "zenith,\t3\r\n",
      "zenith-scouring\t1\r\n",
      "zenith.\t4\r\n",
      "zephyr\t2\r\n",
      "zephyr,\t1\r\n",
      "zephyrs\t3\r\n",
      "zere\t1\r\n",
      "zero\t2\r\n",
      "zero,\t2\r\n",
      "zero.\t1\r\n",
      "zero;\t1\r\n",
      "zest\t8\r\n",
      "zest.\t1\r\n",
      "zest;\t2\r\n",
      "zhentlemans\t1\r\n",
      "ziehe\t1\r\n",
      "ziemlich\t1\r\n",
      "zig-zag\t2\r\n",
      "zigzag\t7\r\n",
      "zigzag;\t1\r\n",
      "zigzagging\t1\r\n",
      "zinc\t2\r\n",
      "zinc-plated\t1\r\n",
      "zinc-plated,\t1\r\n",
      "zis\t5\r\n",
      "zither\t1\r\n",
      "zo\t4\r\n",
      "zone\t4\r\n",
      "zone!\t1\r\n",
      "zones\t4\r\n",
      "zoological\t5\r\n",
      "zoology,\t1\r\n",
      "zu\t20\r\n",
      "zuruck,\t1\r\n",
      "zuruck.\t1\r\n",
      "zuruckkommen\t1\r\n",
      "zusammen\t1\r\n",
      "zusammengetroffen\t1\r\n",
      "zusammengetroffen,\t1\r\n",
      "zuweilen\t1\r\n",
      "zwar\t1\r\n",
      "zwei\t3\r\n",
      "zwolf\t1\r\n",
      "zylobalsamum\t1\r\n",
      "zzip!--let\t1\r\n",
      "{10}\t2\r\n",
      "{13}\t1\r\n",
      "{1}\t14\r\n",
      "{2}\t1\r\n",
      "{2}--Sir\t1\r\n",
      "{3}\t2\r\n",
      "{4}\t2\r\n",
      "{5}\t2\r\n",
      "{6}\t2\r\n",
      "{7}\t2\r\n",
      "{8}\t2\r\n",
      "{9}\t2\r\n",
      "{footnote\t6\r\n",
      "|\t18\r\n",
      "|__________________________________|\t1\r\n",
      "||\t12\r\n",
      "||=======|====\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/salida-wordcount/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
