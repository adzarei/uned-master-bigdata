{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrjob: Palabra más frecuente\n",
    "\n",
    "En este ejemplo vamos a retornar la palabra que tenga más ocurrencias en un fichero de entrada. Para ello será necesario definir un trbaajo que tenga varios pasos MapReduce. Un paso (step) es una combinación de mapper, combiner y reducer, donde ninguno de estas funciones es obligatoria pero debe haber al menos una en cada paso. Mrjob se encarga de encadenar las entradas y salidas de forma transparente al usuario, de forma que se pueden implementar desarrollos más complejos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/palabramasfrecuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/palabramasfrecuente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/palabramasfrecuente\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio.py\n",
    "\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "import re \n",
    " \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") \n",
    "\n",
    "class MRMostUsedWord(MRJob): \n",
    "    def mapper_get_words(self, _, line): \n",
    "        # Para cada palabra en la linea, emitimos un par <palabra, 1> \n",
    "        for word in WORD_RE.findall(line): \n",
    "            yield (word.lower(), 1) \n",
    "\n",
    "    def combiner_count_words(self, word, counts): \n",
    "        # sumamos las palabras que hemos encontrado hasta ahora \n",
    "        yield (word, sum(counts)) \n",
    "\n",
    " \n",
    "    def reducer_count_words(self, word, counts): \n",
    "        # Envia todos los pares < num_ocurrencias , palabra > al mismo reducer,  \n",
    "        # ya que la clave que emitimos es None y es la misma para todos los items \n",
    "        yield None, (sum(counts), word) \n",
    "\n",
    " \n",
    "    # descartamos la clave, es None \n",
    "    def reducer_find_max_word(self, _, word_count_pairs): \n",
    "        # cada item de word_count_pairs es (contador, palabra), \n",
    "        # de forma que emitiendo el maximo nos da la palabra con mas ocurrencias \n",
    "        yield max(word_count_pairs) \n",
    "\n",
    " \n",
    "    def steps(self): \n",
    "        return [ \n",
    "            MRStep(mapper=self.mapper_get_words, \n",
    "                   combiner=self.combiner_count_words, \n",
    "                   reducer=self.reducer_count_words), \n",
    "            MRStep(reducer=self.reducer_find_max_word) \n",
    "        ] \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    MRMostUsedWord.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20190812.104209.840901\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/mrjob-ejercicio.root.20190812.104209.840901/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio.root.20190812.104209.840901/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20190812.104209.840901...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py /media/notebooks/marktwain.txt  > ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155184\t\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "! tail ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/mrjob-palabramasfrecuente-output/_SUCCESS\r\n",
      "Deleted /tmp/carpeta/mrjob-palabramasfrecuente-output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/carpeta/mrjob-palabramasfrecuente-output/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-palabramasfrecuente-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20190812.104945.821766\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.104945.821766/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.104945.821766/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6228870573217200255.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1565590814059_0022\n",
      "  Submitted application application_1565590814059_0022\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1565590814059_0022/\n",
      "  Running job: job_1565590814059_0022\n",
      "  Job job_1565590814059_0022 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1565590814059_0022 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.104945.821766/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=16018031\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1074649\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1114899\n",
      "\t\tFILE: Number of bytes written=2676356\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=16018227\n",
      "\t\tHDFS: Number of bytes written=1074649\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35142656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3452928\n",
      "\t\tTotal time spent by all map tasks (ms)=34319\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34319\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3372\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3372\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34319\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3372\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=12280\n",
      "\t\tCombine input records=2845161\n",
      "\t\tCombine output records=75130\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=169\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=302272\n",
      "\t\tMap output bytes=26306867\n",
      "\t\tMap output materialized bytes=1114905\n",
      "\t\tMap output records=2845161\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=828125184\n",
      "\t\tReduce input groups=51215\n",
      "\t\tReduce input records=75130\n",
      "\t\tReduce output records=51215\n",
      "\t\tReduce shuffle bytes=1114905\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=150260\n",
      "\t\tTotal committed heap usage (bytes)=776470528\n",
      "\t\tVirtual memory (bytes) snapshot=7912366080\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6629286745543115521.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1565590814059_0023\n",
      "  Submitted application application_1565590814059_0023\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1565590814059_0023/\n",
      "  Running job: job_1565590814059_0023\n",
      "  Job job_1565590814059_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1565590814059_0023 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-palabramasfrecuente-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1078745\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1177085\n",
      "\t\tFILE: Number of bytes written=2799081\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1079073\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5671936\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2626560\n",
      "\t\tTotal time spent by all map tasks (ms)=5539\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5539\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2565\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2565\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5539\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2565\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2320\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=193\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=51215\n",
      "\t\tMap output bytes=1074649\n",
      "\t\tMap output materialized bytes=1177091\n",
      "\t\tMap output records=51215\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=807067648\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=51215\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=1177091\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=102430\n",
      "\t\tTotal committed heap usage (bytes)=770703360\n",
      "\t\tVirtual memory (bytes) snapshot=7916670976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-palabramasfrecuente-output\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190812.104945.821766...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20190812.104945.821766...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py hdfs:///tmp/carpeta/marktwain.txt -r hadoop --python-bin /opt/anaconda/bin/python3.7 \\\n",
    "--output-dir hdfs:///tmp/carpeta/mrjob-palabramasfrecuente-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155184\t\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/carpeta/mrjob-palabramasfrecuente-output/part-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
