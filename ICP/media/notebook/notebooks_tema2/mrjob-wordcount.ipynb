{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrjob: Contador de palabras\n",
    "\n",
    "En este ejemplo trabajaremos con un fichero de texto cargado previamente en HDFS. Haremos primero una ejecuci√≥n en local y luego en Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /media/notebook/notebooks_tema2/mrjob/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebook/notebooks_tema2/mrjob/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebook/notebooks_tema2/mrjob/wordcount\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio.py\n",
    "from mrjob.job import MRJob \n",
    "\n",
    "import re \n",
    "\n",
    "# preparamos una expresion regular que recoja las palabras.  \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") \n",
    "\n",
    "class MRWordFreqCount(MRJob): \n",
    "\n",
    "    def mapper(self, _, line): \n",
    "       # Para cada palabra en la linea, emitimos un par <palabra, 1> \n",
    "        for word in WORD_RE.findall(line): \n",
    "            yield (word.lower(), 1) \n",
    "\n",
    "    # El combiner agrega los pares <palabra, 1> que se emitan en el mismo map. \n",
    "    def combiner(self, word, counts): \n",
    "        yield (word, sum(counts)) \n",
    "\n",
    "    #El reducer agrega los pares <palabra, X> que le llegan \n",
    "    def reducer(self, word, counts): \n",
    "        yield (word, sum(counts)) \n",
    "\n",
    "if __name__ == '__main__': \n",
    "     MRWordFreqCount.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20191103.181729.125226\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/mrjob-ejercicio.root.20191103.181729.125226/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio.root.20191103.181729.125226/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20191103.181729.125226...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py /media/notebook/datos/marktwain.txt  > outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"texto\"\t5\r\n",
      "\"todo\"\t1\r\n",
      "\"toma\"\t1\r\n",
      "\"un\"\t2\r\n",
      "\"zapater\"\t1\r\n",
      "\"es\"\t1\r\n",
      "\"escribiendo\"\t1\r\n",
      "\"esto\"\t1\r\n",
      "\"estoy\"\t1\r\n",
      "\"hola\"\t2\r\n",
      "\"mas\"\t4\r\n",
      "\"nuevo\"\t1\r\n",
      "\"palabras\"\t1\r\n",
      "\"para\"\t1\r\n",
      "\"saludo\"\t1\r\n",
      "\"tenemos\"\t1\r\n",
      "\"test\"\t1\r\n",
      "\"adri\\u00e1n\"\t1\r\n",
      "\"bien\"\t1\r\n",
      "\"contar\"\t1\r\n",
      "\"cordial\"\t1\r\n",
      "\"cuantas\"\t1\r\n",
      "\"de\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "! cat outputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/mrjob-wordcount-output/_SUCCESS\r\n",
      "Deleted /tmp/carpeta/mrjob-wordcount-output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/carpeta/mrjob-wordcount-output/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-wordcount-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20191103.181737.130894\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20191103.181737.130894/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20191103.181737.130894/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8902058658768382756.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.3:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1572803728648_0002\n",
      "  Submitted application application_1572803728648_0002\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1572803728648_0002/\n",
      "  Running job: job_1572803728648_0002\n",
      "  Job job_1572803728648_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1572803728648_0002 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-wordcount-output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=302\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=237\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=298\n",
      "\t\tFILE: Number of bytes written=447082\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=544\n",
      "\t\tHDFS: Number of bytes written=237\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8674304\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3504128\n",
      "\t\tTotal time spent by all map tasks (ms)=8471\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8471\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3422\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3422\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8471\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3422\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1460\n",
      "\t\tCombine input records=32\n",
      "\t\tCombine output records=24\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=664\n",
      "\t\tInput split bytes=242\n",
      "\t\tMap input records=9\n",
      "\t\tMap output bytes=317\n",
      "\t\tMap output materialized bytes=304\n",
      "\t\tMap output records=32\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=930275328\n",
      "\t\tReduce input groups=23\n",
      "\t\tReduce input records=24\n",
      "\t\tReduce output records=23\n",
      "\t\tReduce shuffle bytes=304\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=48\n",
      "\t\tTotal committed heap usage (bytes)=915931136\n",
      "\t\tVirtual memory (bytes) snapshot=7886835712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-wordcount-output\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20191103.181737.130894...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20191103.181737.130894...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py hdfs:///tmp/carpeta/mr-job-wordcount-input/marktwain.txt -r hadoop --python-bin \\\n",
    "/opt/anaconda/bin/python3.7 --output-dir /tmp/carpeta/mrjob-wordcount-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2019-11-03 18:18 /tmp/carpeta/mrjob-wordcount-output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup        237 2019-11-03 18:18 /tmp/carpeta/mrjob-wordcount-output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/carpeta/mrjob-wordcount-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"adri\\u00e1n\"\t1\r\n",
      "\"bien\"\t1\r\n",
      "\"contar\"\t1\r\n",
      "\"cordial\"\t1\r\n",
      "\"cuantas\"\t1\r\n",
      "\"de\"\t1\r\n",
      "\"es\"\t1\r\n",
      "\"escribiendo\"\t1\r\n",
      "\"esto\"\t1\r\n",
      "\"estoy\"\t1\r\n",
      "\"hola\"\t2\r\n",
      "\"mas\"\t4\r\n",
      "\"nuevo\"\t1\r\n",
      "\"palabras\"\t1\r\n",
      "\"para\"\t1\r\n",
      "\"saludo\"\t1\r\n",
      "\"tenemos\"\t1\r\n",
      "\"test\"\t1\r\n",
      "\"texto\"\t5\r\n",
      "\"todo\"\t1\r\n",
      "\"toma\"\t1\r\n",
      "\"un\"\t2\r\n",
      "\"zapater\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/carpeta/mrjob-wordcount-output/part-00000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
