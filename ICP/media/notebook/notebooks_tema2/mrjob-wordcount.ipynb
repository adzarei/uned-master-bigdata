{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrjob: Contador de palabras\n",
    "\n",
    "En este ejemplo trabajaremos con un fichero de texto cargado previamente en HDFS. Haremos primero una ejecuci√≥n en local y luego en Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/wordcount\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio.py\n",
    "from mrjob.job import MRJob \n",
    "\n",
    "import re \n",
    "\n",
    "# preparamos una expresion regular que recoja las palabras.  \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") \n",
    "\n",
    "class MRWordFreqCount(MRJob): \n",
    "\n",
    "    def mapper(self, _, line): \n",
    "       # Para cada palabra en la linea, emitimos un par <palabra, 1> \n",
    "        for word in WORD_RE.findall(line): \n",
    "            yield (word.lower(), 1) \n",
    "\n",
    "    # El combiner agrega los pares <palabra, 1> que se emitan en el mismo map. \n",
    "    def combiner(self, word, counts): \n",
    "        yield (word, sum(counts)) \n",
    "\n",
    "    #El reducer agrega los pares <palabra, X> que le llegan \n",
    "    def reducer(self, word, counts): \n",
    "        yield (word, sum(counts)) \n",
    "\n",
    "if __name__ == '__main__': \n",
    "     MRWordFreqCount.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20190726.103809.511889\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/mrjob-ejercicio.root.20190726.103809.511889/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio.root.20190726.103809.511889/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20190726.103809.511889...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py /media/notebooks/marktwain.txt  > ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"unpatriotism\"\t1\r\n",
      "\"unpaved\"\t2\r\n",
      "\"unpeopled\"\t7\r\n",
      "\"unperfected\"\t3\r\n",
      "\"unpestered\"\t1\r\n",
      "\"unphilosophical\"\t1\r\n",
      "\"unpictured\"\t1\r\n",
      "\"unpicturesque\"\t3\r\n",
      "\"unpinned\"\t2\r\n",
      "\"unpiratical\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "! tail ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/carpeta/mrjob-wordcount-output/*': No such file or directory\n",
      "rmdir: `/tmp/carpeta/mrjob-wordcount-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/carpeta/mrjob-wordcount-output/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-wordcount-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20190726.103929.243950\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190726.103929.243950/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190726.103929.243950/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6536794343884990512.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.28.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.28.0.4:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1564135641550_0004\n",
      "  Submitted application application_1564135641550_0004\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1564135641550_0004/\n",
      "  Running job: job_1564135641550_0004\n",
      "  Job job_1564135641550_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1564135641550_0004 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-wordcount-output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=15715759\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=664929\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1114898\n",
      "\t\tFILE: Number of bytes written=2676213\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=15715955\n",
      "\t\tHDFS: Number of bytes written=664929\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=57485312\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3486720\n",
      "\t\tTotal time spent by all map tasks (ms)=56138\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=56138\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3405\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3405\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=56138\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3405\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=41820\n",
      "\t\tCombine input records=2845161\n",
      "\t\tCombine output records=75130\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=154\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=302272\n",
      "\t\tMap output bytes=26306867\n",
      "\t\tMap output materialized bytes=1114904\n",
      "\t\tMap output records=2845161\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=893145088\n",
      "\t\tReduce input groups=51215\n",
      "\t\tReduce input records=75130\n",
      "\t\tReduce output records=51215\n",
      "\t\tReduce shuffle bytes=1114904\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=150260\n",
      "\t\tTotal committed heap usage (bytes)=835190784\n",
      "\t\tVirtual memory (bytes) snapshot=7972208640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-wordcount-output\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20190726.103929.243950...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20190726.103929.243950...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio.py hdfs:///tmp/carpeta/marktwain.txt -r hadoop --python-bin /opt/anaconda/bin/python3.7 \\\n",
    "--output-dir /tmp/carpeta/mrjob-wordcount-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2019-07-26 10:40 /tmp/carpeta/mrjob-wordcount-output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup     664929 2019-07-26 10:40 /tmp/carpeta/mrjob-wordcount-output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/carpeta/mrjob-wordcount-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\"\t2\r\n",
      "\"zat\"\t2\r\n",
      "\"ze\"\t24\r\n",
      "\"zeal\"\t36\r\n",
      "\"zealand\"\t59\r\n",
      "\"zealand's\"\t1\r\n",
      "\"zealander\"\t1\r\n",
      "\"zealous\"\t10\r\n",
      "\"zealously\"\t4\r\n",
      "\"zeb\"\t3\r\n",
      "\"zebra\"\t2\r\n",
      "\"zebras\"\t2\r\n",
      "\"zebulon\"\t1\r\n",
      "\"zedekiah\"\t1\r\n",
      "\"zedoary\"\t1\r\n",
      "\"zehn\"\t1\r\n",
      "\"zei\"\t1\r\n",
      "\"zeilerus\"\t1\r\n",
      "\"zeit\"\t3\r\n",
      "\"zeitung\"\t3\r\n",
      "\"zeitung's\"\t2\r\n",
      "\"zeitvertreib\"\t1\r\n",
      "\"zenana\"\t8\r\n",
      "\"zeniff\"\t1\r\n",
      "\"zenith\"\t17\r\n",
      "\"zenobia\"\t2\r\n",
      "\"zenophon\"\t1\r\n",
      "\"zephyr\"\t7\r\n",
      "\"zephyrs\"\t3\r\n",
      "\"zere\"\t1\r\n",
      "\"zermatt\"\t37\r\n",
      "\"zero\"\t6\r\n",
      "\"zest\"\t11\r\n",
      "\"zeus\"\t2\r\n",
      "\"zeuxis\"\t1\r\n",
      "\"zhentlemans\"\t1\r\n",
      "\"ziani\"\t1\r\n",
      "\"ziehe\"\t1\r\n",
      "\"ziemlich\"\t1\r\n",
      "\"zig\"\t2\r\n",
      "\"zigzag\"\t8\r\n",
      "\"zigzagging\"\t1\r\n",
      "\"zimmermanns\"\t1\r\n",
      "\"zimmern\"\t1\r\n",
      "\"zinc\"\t5\r\n",
      "\"zion\"\t3\r\n",
      "\"zip\"\t1\r\n",
      "\"zis\"\t9\r\n",
      "\"zither\"\t4\r\n",
      "\"zo\"\t4\r\n",
      "\"zoe\"\t4\r\n",
      "\"zoe's\"\t1\r\n",
      "\"zogbaum\"\t1\r\n",
      "\"zola\"\t1\r\n",
      "\"zone\"\t5\r\n",
      "\"zones\"\t5\r\n",
      "\"zonoras\"\t5\r\n",
      "\"zoo\"\t3\r\n",
      "\"zoological\"\t10\r\n",
      "\"zoology\"\t1\r\n",
      "\"zorah\"\t1\r\n",
      "\"zorn\"\t1\r\n",
      "\"zoroaster\"\t2\r\n",
      "\"zoroastrian\"\t1\r\n",
      "\"zoroastrians\"\t3\r\n",
      "\"zouch\"\t1\r\n",
      "\"zu\"\t21\r\n",
      "\"zufallig\"\t1\r\n",
      "\"zug\"\t8\r\n",
      "\"zuge\"\t1\r\n",
      "\"zugs\"\t1\r\n",
      "\"zulu\"\t9\r\n",
      "\"zulus\"\t8\r\n",
      "\"zum\"\t2\r\n",
      "\"zunge\"\t1\r\n",
      "\"zurich\"\t2\r\n",
      "\"zuruck\"\t2\r\n",
      "\"zuruckkommen\"\t1\r\n",
      "\"zusammen\"\t1\r\n",
      "\"zusammengetroffen\"\t2\r\n",
      "\"zutphen\"\t1\r\n",
      "\"zuweilen\"\t2\r\n",
      "\"zwar\"\t1\r\n",
      "\"zwei\"\t3\r\n",
      "\"zwinglian\"\t1\r\n",
      "\"zwischen\"\t1\r\n",
      "\"zwolf\"\t1\r\n",
      "\"zy\"\t1\r\n",
      "\"zylo\"\t2\r\n",
      "\"zylobalsamum\"\t5\r\n",
      "\"zzip\"\t1\r\n",
      "\"zzz\"\t10\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/carpeta/mrjob-wordcount-output/part-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
