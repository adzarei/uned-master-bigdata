{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de patrones: TopN\n",
    "\n",
    "En este ejercicio partimos de un dataset que contiene posts enviados a un foro de Internet. Se pide filtrar los posts, devuelva los N posts cuyos contenidos sean los más largos.\n",
    "\n",
    "Un post contiene una serie de campos separados por tabuladores. Los campos son los siguientes: \n",
    "\n",
    "\"id\", \"title\", \"tagnames\", \"author_id\", \"body\", \"node_type\", \"parent_id\", \"abs_parent_id\", \"added_at\", \"score\", \"state_string\", \"last_edited_id\", \"last_activity_by_id\", \"last_activity_at\", \"active_revision_id\", \"extra\", \"extra_ref_id\", \"extra_count\", \"marked\" \n",
    "\n",
    "Para este ejercicio solamente nos interesa el campo \"body\" que contiene el texto del post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ejemplo-patrones/topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/ejemplo-patrones/topN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/ejemplo-patrones/topN\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "reader = csv.reader(sys.stdin, delimiter='\\t')\n",
    "\n",
    "mykey = None # inventado\n",
    "\n",
    "topN = 2\n",
    "\n",
    "#Una lista que incluye cada linea y su longitud en forma de sublista\n",
    "lineWithLength=[]\n",
    "\n",
    "# PON TU CODIGO AQUÍ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "topN_Value = 2\n",
    "\n",
    "lineWithLength=[]\n",
    "\n",
    "# PON TU CODIGO AQUÍ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 root supergroup       1774 2019-08-12 08:03 /tmp/forumdata/forum1.tsv\r\n",
      "-rw-r--r--   3 root supergroup  120313812 2019-08-12 08:03 /tmp/forumdata/forum_node.tsv\r\n",
      "-rw-r--r--   3 root supergroup     530989 2019-08-12 08:03 /tmp/forumdata/forum_users.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/forumdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda realiza una prueba con un fichero de posts corto llamado forum1.tsv. Asi se ve facilmente que el codigo funciona bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/salida-topNTest/*': No such file or directory\n",
      "rmdir: `/tmp/salida-topNTest': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/salida-topNTest/*\n",
    "! hdfs dfs -rmdir /tmp/salida-topNTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob814665353921995048.jar tmpDir=null\n",
      "19/08/12 09:58:09 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "19/08/12 09:58:09 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "19/08/12 09:58:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/08/12 09:58:10 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/08/12 09:58:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1565590814059_0017\n",
      "19/08/12 09:58:10 INFO impl.YarnClientImpl: Submitted application application_1565590814059_0017\n",
      "19/08/12 09:58:10 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1565590814059_0017/\n",
      "19/08/12 09:58:10 INFO mapreduce.Job: Running job: job_1565590814059_0017\n",
      "19/08/12 09:58:15 INFO mapreduce.Job: Job job_1565590814059_0017 running in uber mode : false\n",
      "19/08/12 09:58:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/08/12 09:58:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/08/12 09:58:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/08/12 09:58:25 INFO mapreduce.Job: Job job_1565590814059_0017 completed successfully\n",
      "19/08/12 09:58:25 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1073\n",
      "\t\tFILE: Number of bytes written=444639\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2855\n",
      "\t\tHDFS: Number of bytes written=517\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5530\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2115\n",
      "\t\tTotal time spent by all map tasks (ms)=5530\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2115\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5530\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2115\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5662720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2165760\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=1053\n",
      "\t\tMap output materialized bytes=1079\n",
      "\t\tInput split bytes=194\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=1079\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=188\n",
      "\t\tCPU time spent (ms)=1270\n",
      "\t\tPhysical memory (bytes) snapshot=797843456\n",
      "\t\tVirtual memory (bytes) snapshot=7917383680\n",
      "\t\tTotal committed heap usage (bytes)=797442048\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2661\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=517\n",
      "19/08/12 09:58:25 INFO streaming.StreamJob: Output directory: /tmp/salida-topNTest\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-files mapper.py,reducer.py  -mapper mapper.py -reducer reducer.py \\\n",
    "-input /tmp/forumdata/forum1.tsv -output /tmp/salida-topNTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0206', 'Titulo', 'tags', '9191', 'Este es un mensaje de 5\\\\n  lineas\\\\n pero\\\\n solo \\\\n 1 frase.', 'question', '\\\\N', '\\\\N', '2012-02-27 15:09:11.184434+00', '0', '', '\\\\N', '100003268', '2012-02-27 15:09:11.184434+00', '9322', '\\\\N', '\\\\N', '106', 'f']\t\r\n",
      "['0207', 'Titulo', 'tags', '9191', 'Este es\\\\n un mensaje de 6\\\\n lineas\\\\n pero \\\\nsolo 1 \\\\nfrase.', 'question', '\\\\N', '\\\\N', '2012-02-27 15:09:11.184434+00', '0', '', '\\\\N', '100003268', '2012-02-27 15:09:11.184434+00', '9322', '\\\\N', '\\\\N', '106', 'f']\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/salida-topNTest/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/salida-topN/*': No such file or directory\n",
      "rmdir: `/tmp/salida-topN': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/salida-topN/*\n",
    "! hdfs dfs -rmdir /tmp/salida-topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob1439647105550458621.jar tmpDir=null\n",
      "19/08/12 09:59:15 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "19/08/12 09:59:15 INFO client.RMProxy: Connecting to ResourceManager at yarnmaster/172.21.0.3:8032\n",
      "19/08/12 09:59:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/08/12 09:59:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/08/12 09:59:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1565590814059_0018\n",
      "19/08/12 09:59:16 INFO impl.YarnClientImpl: Submitted application application_1565590814059_0018\n",
      "19/08/12 09:59:16 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1565590814059_0018/\n",
      "19/08/12 09:59:16 INFO mapreduce.Job: Running job: job_1565590814059_0018\n",
      "19/08/12 09:59:21 INFO mapreduce.Job: Job job_1565590814059_0018 running in uber mode : false\n",
      "19/08/12 09:59:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/08/12 09:59:27 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/08/12 09:59:28 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/08/12 09:59:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/08/12 09:59:32 INFO mapreduce.Job: Job job_1565590814059_0018 completed successfully\n",
      "19/08/12 09:59:32 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=232248\n",
      "\t\tFILE: Number of bytes written=906992\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=120318110\n",
      "\t\tHDFS: Number of bytes written=150354\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8846\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2263\n",
      "\t\tTotal time spent by all map tasks (ms)=8846\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2263\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8846\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2263\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9058304\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2317312\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=922570\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=232224\n",
      "\t\tMap output materialized bytes=232254\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=232254\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=167\n",
      "\t\tCPU time spent (ms)=3240\n",
      "\t\tPhysical memory (bytes) snapshot=777609216\n",
      "\t\tVirtual memory (bytes) snapshot=7916912640\n",
      "\t\tTotal committed heap usage (bytes)=770179072\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=120317908\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=150354\n",
      "19/08/12 09:59:32 INFO streaming.StreamJob: Output directory: /tmp/salida-topN\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /tmp/forumdata/forum_node.tsv -output /tmp/salida-topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bundle/Contents/Resources/google_appengine/google/appengine/api/validation.py:73: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6<br>\\n  self.message = message<br>\\n/Users/George/Desktop/GoogleAppEngineLauncher.app/Contents/Resources/GoogleAppEngine-default.bundle/Contents/Resources/google_appengine/google/appengine/api/validation.py:77: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6<br>\\n  return str(self.message)<br>\\nError parsing yaml file:<br>\\nInvalid object:<br>\\nthreadsafe cannot be enabled with CGI handler: helloworld.py<br>\\n  in \"/Users/George/udacity1/app.yaml\", line 9, column 24<br>\\nIf deploy fails you might need to \\'rollback\\' manually.<br>\\nThe \"Make Symlinks...\" menu option can help with command-line work.</em></strong> appcfg.py has finished with exit code 1 ***</p>', 'answer', '6010539', '6010539', '2012-04-28 22:00:17.081585+00', '0', '', '\\\\N', '100079746', '2012-04-28 22:00:17.081585+00', '6013214', '\\\\N', '\\\\N', '0', 'f']\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/salida-topN/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
